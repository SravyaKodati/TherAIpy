# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tqBHYrhtw32U_-stYJNXreRsWsnHyXM4
"""

pip install pymupdf
pip install faiss-gpu

import fitz
import json
import faiss
import numpy as np
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer

def extract_text_pymupdf(pdf_path, start_page=7):
    """
    Extracts text from a PDF file using PyMuPDF.
    Args:
        pdf_path (str): Path to the PDF file.
    Returns:
        str: Extracted text from the PDF.
    """
    text = ""
    try:
        pdf_document = fitz.open(pdf_path)
        for page_num in range(start_page, len(pdf_document)):
            page = pdf_document[page_num]
            text += page.get_text()
    except Exception as e:
        print(f"Error reading {pdf_path}: {e}")
    return text


def clean_text(text):
    """
    Cleans the extracted text by removing extra spaces, newlines, and unwanted characters.
    Args:
        text (str): The raw extracted text.
    Returns:
        str: Cleaned text.
    """
    cleaned_text = text.replace("\n", " ").replace("\r", " ")
    cleaned_text = " ".join(cleaned_text.split())
    return cleaned_text


def chunk_text(text, tokenizer, max_tokens=108):
    """
    Splits text into chunks based on the token limit of a tokenizer.
    Args:
        text (str): The input text to be chunked.
        tokenizer_name (str): The tokenizer model to use.
        max_tokens (int): Maximum tokens allowed per chunk.
    Returns:
        list: A list of text chunks.
    """

    words = text.split()
    chunks = []
    current_chunk = []
    current_tokens = 0

    for word in words:
        word_tokens = tokenizer.encode(word, add_special_tokens=False)
        if current_tokens + len(word_tokens) > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_tokens = 0
        current_chunk.append(word)
        current_tokens += len(word_tokens)

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


def generate_embeddings(chunks, model_name="all-MiniLM-L6-v2"):
    """
    Generates embeddings for text chunks using a SentenceTransformer model.
    Args:
        chunks (list): List of text chunks.
        model_name (str): Name of the embedding model.
    Returns:
        list: A list of embeddings.
    """
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks, show_progress_bar=True)
    return embeddings


def create_faiss_index(embeddings, d):
    """
    Creates a FAISS index from embeddings.
    Args:
        embeddings (numpy.ndarray): The embeddings matrix.
        d (int): Dimension of the embeddings.
    Returns:
        faiss.IndexFlatL2: The FAISS index.
    """
    index = faiss.IndexFlatL2(d)  # L2 distance
    index.add(embeddings)
    return index


def process_and_store(book_path, tokenizer, embedding_model, faiss_index, metadata_file):
    """
    Processes a book, chunks it, generates embeddings, and adds them to a FAISS index.
    Updates metadata dynamically in a JSON file.

    Args:
        book_path (str): Path to the book (PDF).
        tokenizer: Tokenizer to calculate token limits.
        embedding_model: Model to generate embeddings.
        faiss_index: FAISS index to store embeddings.
        metadata_file (str): Path to the metadata JSON file.
    """
    text = extract_text_pymupdf(book_path)

    chunks = chunk_text(text, tokenizer, max_tokens=500)
    print(f"Number of chunks: {len(chunks)}")
    print(f"First chunk: {chunks[0]}")

    try:
        with open(metadata_file, "r") as f:
            metadata = json.load(f)
    except FileNotFoundError:
        metadata = {}

    for chunk_idx, chunk in enumerate(chunks):
        embedding = embedding_model.encode([chunk])
        faiss_index.add(embedding)

        global_idx = len(metadata)
        metadata[global_idx] = {
            "book": book_path,
            "chunk_number": chunk_idx,
            "text": chunk
        }

    with open(metadata_file, "w") as f:
        json.dump(metadata, f, indent=4)


initial_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

d = 384
faiss_index = faiss.IndexFlatL2(d)

metadata_file = "metadata.json"

books = ["b1.pdf", "b2.pdf", "b3.pdf", "b4.pdf", "b5.pdf", "b6.pdf", "b7.pdf", "b8.pdf"]
for book in books:
    process_and_store(book, initial_tokenizer, embedding_model, faiss_index, metadata_file)

faiss.write_index(faiss_index, "faiss_index")